{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd1f3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'Access-Control-Allow-Origin': '*',\n",
    "    'Access-Control-Allow-Methods': 'GET',\n",
    "    'Access-Control-Allow-Headers': 'Content-Type',\n",
    "    'Access-Control-Max-Age': '3600',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "\n",
    "url = 'https://companiesmarketcap.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7a7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earnings(url):\n",
    "    req = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "    \n",
    "    #Earnings\n",
    "    earnings = dom.xpath('//h2/strong/span')\n",
    "    if earnings:\n",
    "        return earnings[0].text\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_revenue(url):\n",
    "    req = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "    \n",
    "    #Earnings\n",
    "    revenue = dom.xpath('//h2/strong/span')\n",
    "    if revenue:\n",
    "        return revenue[0].text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_shares(url):\n",
    "    req = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "    \n",
    "    #Earnings\n",
    "    shares = dom.xpath('//h2/strong/span')\n",
    "    if shares:\n",
    "        shares_txt = shares[0].text\n",
    "        shares_txt = shares_txt.replace(',', '.')\n",
    "        return shares_txt\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def save_category(cat, id_azienda, writer):\n",
    "    cat_name = re.sub(r'[^a-zA-Z0-9]', '', cat)\n",
    "    row = [id_azienda, cat_name, hash(cat_name)]    \n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e490fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_marketcap(url2, employees, id_, writer_a, writer_c):\n",
    "    pre_req = time.time()\n",
    "    req = requests.get(url2, headers)\n",
    "    post_req = time.time()\n",
    "    tot_req = post_req - pre_req\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "    \n",
    "    global shares2\n",
    "    \n",
    "    #Nome\n",
    "    pre_name = time.time()\n",
    "    name = dom.xpath('//div[@class = \"company-name\"]')\n",
    "    post_name = time.time()\n",
    "    tot_name = post_name -pre_name\n",
    "    if name:\n",
    "        name_txt = name[0].text\n",
    "        name_txt = name_txt.replace('\"', '')\n",
    "        name_txt = name_txt.replace('\\n', '')\n",
    "    else:\n",
    "        name_txt = None\n",
    "    \n",
    "    #Marketcap\n",
    "    pre_mc = time.time()\n",
    "    mc = dom.xpath('//div[text() = \"Marketcap\"]/../div[1]')\n",
    "    post_mc = time.time()\n",
    "    tot_mc = post_mc - pre_mc\n",
    "    if mc:\n",
    "        mc_txt = mc[0].text\n",
    "    else:\n",
    "        mc_txt = none\n",
    "    \n",
    "    #Share price\n",
    "    pre_sp = time.time()\n",
    "    sp = dom.xpath('//div[text() = \"Share price\"]/../div[1]')\n",
    "    post_sp = time.time()\n",
    "    tot_sp = post_sp - pre_sp\n",
    "    if sp:\n",
    "        sp_txt = sp[0].text\n",
    "    else:\n",
    "        sp = None\n",
    "    \n",
    "    #Company code\n",
    "    pre_cc = time.time()\n",
    "    cc = dom.xpath('//div[@class = \"company-code\"]')\n",
    "    post_cc = time.time()\n",
    "    tot_cc = post_cc - pre_cc\n",
    "    if cc:\n",
    "        cc_txt = cc[0].text\n",
    "    else:\n",
    "        cc_txt = None\n",
    "        \n",
    "    #Revenue e Earnings\n",
    "    pre_links = time.time()\n",
    "    links_1 = dom.xpath('//a[@class = \"nav-link\"]/@href')\n",
    "    post_links = time.time()\n",
    "    tot_links = post_links - pre_links\n",
    "    for link in links_1:\n",
    "        if '/earnings' in link:\n",
    "            earnings = get_earnings(url + link)\n",
    "        elif 'revenue' in link:\n",
    "            revenue = get_revenue(url + link)\n",
    "    \n",
    "    #Number of shares\n",
    "    pre_shares = time.time()\n",
    "    links_2 = dom.xpath('//div[@class = \"dropdown-menu\"]/a/@href')\n",
    "    post_shares = time.time()\n",
    "    tot_shares = post_shares - pre_shares\n",
    "    for link in links_2:\n",
    "        if '/shares-outstanding' in link:\n",
    "            shares2 = get_shares(url + link)\n",
    "            \n",
    "    #Categories\n",
    "    pre_cat = time.time()\n",
    "    categories = dom.xpath('//*[@id=\"cmkt\"]/div[3]/div[1]/div[2]/div[3]/div[1]/a/text()')\n",
    "    post_cat = time.time()\n",
    "    tot_cat = post_cat - pre_cat\n",
    "    for cat in categories:\n",
    "        save_category(cat, id_, writer_c)\n",
    "    \n",
    "    \n",
    "    employees = employees.replace(',', '.')\n",
    "\n",
    "    row = [url2, id_, name_txt, cc_txt, mc_txt, sp_txt, earnings, revenue, shares2, employees, tot_req, tot_name,\n",
    "          tot_cc, tot_mc, tot_sp, tot_links, tot_shares, tot_cat]\n",
    "    writer_a.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931a2fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "1079.712150335312\n",
      "{1: [0.40468597412109375, 0.5542092323303223], 2: [0.18917369842529297, 0.1410374641418457], 3: [0.2138669490814209, 0.22008109092712402], 4: [0.20974493026733398, 0.23383021354675293], 5: [0.19745993614196777, 0.2245321273803711], 6: [0.21550464630126953, 0.16414332389831543], 7: [0.2039964199066162, 0.16255545616149902], 8: [0.2192673683166504, 0.15709710121154785], 9: [0.22065234184265137, 0.16556143760681152], 10: [0.23503589630126953, 0.1490774154663086], 11: [0.3730180263519287, 0.23321938514709473], 12: [0.2534956932067871, 0.1380774974822998], 13: [0.23784899711608887, 0.1911003589630127], 14: [0.24958205223083496, 0.2367093563079834]}\n",
      "1400 aziende\n"
     ]
    }
   ],
   "source": [
    "ds_a = open('cmc-aziende-dataset.csv', 'w')\n",
    "head_a = ['URL', 'ID', 'Name', 'Company code', 'Marketcap', 'Share price', \n",
    "        'Earnings', 'Revenue', 'Shares','Employees', 'Tempo request', 'Tempo estrazione Name', 'Tempo estrazione CC',\n",
    "        'Tempo estrazione marketcap', 'Tempo estrazione Share price', 'Tempo estrazione Revenue/Earnings', \n",
    "        'Tempo estrazione Shares', 'Tempo estrazione Categories']\n",
    "writer_a = csv.writer(ds_a)\n",
    "writer_a.writerow(head_a)\n",
    "\n",
    "ds_c = open('cmc-categorie-dataset.csv', 'w')\n",
    "head_c = ['Company ID', 'Category', 'Hash']\n",
    "writer_c = csv.writer(ds_c)\n",
    "writer_c.writerow(head_c)\n",
    "\n",
    "d = {}\n",
    "\n",
    "id_ = 0\n",
    "start = time.time()\n",
    "for x in range(1, 15):     #(1, 15)\n",
    "    start_req = time.time()\n",
    "    req = requests.get(url + '/usa/largest-american-companies-by-number-of-employees/?page=' + str(x), headers)\n",
    "    end_req = time.time()\n",
    "    tot_req = end_req - start_req\n",
    "    start_est = time.time()\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "    links = dom.xpath('//a[contains(@href, \"/marketcap\")]/@href')\n",
    "    employees = dom.xpath('//td[3]')\n",
    "    end_est = time.time()\n",
    "    tot_est = end_est - start_est\n",
    "    d[x] = [tot_req, tot_est]\n",
    "    print(x)\n",
    "    for n in range(0, 100):   # (0, 100)\n",
    "        id_ += 1\n",
    "        url2 = url + links[n]\n",
    "        get_info_marketcap(url2, employees[n].text, id_, writer_a, writer_c)\n",
    "end = time.time();\n",
    "ds_a.close()\n",
    "ds_c.close()\n",
    "\n",
    "print(end - start)\n",
    "print(d)\n",
    "print(id_, 'aziende')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72398d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "992840e6a8ef67cde68ee6e60e6da10c41df61722c19d4845d9b07a7cc58a686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
